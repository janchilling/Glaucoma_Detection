{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO5FFcU2TjAQJrqrV78DCGH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJ3nBgb25s7W"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r ~/.kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!mv ./kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "YugM_6OT6eVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d deathtrooper/multichannel-glaucoma-benchmark-dataset"
      ],
      "metadata": {
        "id": "TTEwfNiEMYV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq multichannel-glaucoma-benchmark-dataset.zip"
      ],
      "metadata": {
        "id": "zb0TBkW1jihU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataframe = pd.read_csv(\"/content/metadata - standardized.csv\")\n",
        "dataframe.head()"
      ],
      "metadata": {
        "id": "bADNE0tikqM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_count = sum([i for i in dataframe[\"types\"] == 1])\n",
        "negative_count = sum([i for i in dataframe[\"types\"] == 0])\n",
        "suspect_count = sum([i for i in dataframe[\"types\"] == -1])\n",
        "\n",
        "print(f\"Number of Glaucoma positive cases in the dataset = {positive_count}\")\n",
        "print(f\"Number if Glaucoma negative cases in the dataset = {negative_count}\")\n",
        "print(f\"Number of Glaucoma suspect cases in the dataset = {suspect_count}\")"
      ],
      "metadata": {
        "id": "etSMozjglVcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "\n",
        "original_fundus_dir = pathlib.Path(\"full-fundus/full-fundus\")"
      ],
      "metadata": {
        "id": "N3DQZOTcD4dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orginal_dir_all_images = len([images for images in original_fundus_dir.glob(\"*\")])\n",
        "orginal_dir_all_images"
      ],
      "metadata": {
        "id": "NjBiF7pNEK9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "image_path = os.path.join(original_fundus_dir, \"OIA-ODIR-TEST-OFFLINE-1.png\")\n",
        "\n",
        "if os.path.exists(image_path):\n",
        "    print(f\"Image exists\")\n",
        "else:\n",
        "    print(f\"Image does not exist\")"
      ],
      "metadata": {
        "id": "OjneHKyUE766"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "categorized_dir_neg = pathlib.Path(\"Fundus/Negative\")\n",
        "categorized_dir_pos = pathlib.Path(\"Fundus/Positive\")\n",
        "\n",
        "os.makedirs(categorized_dir_neg, exist_ok = True)\n",
        "os.makedirs(categorized_dir_pos, exist_ok = True)\n",
        "\n",
        "for index, row in dataframe.iterrows():\n",
        "  src_path = \"full-fundus\" + row[\"fundus\"]\n",
        "\n",
        "  if row[\"types\"] == 0:\n",
        "    dst_path = categorized_dir_neg / pathlib.Path(row[\"fundus\"]).name\n",
        "  elif row[\"types\"] == 1:\n",
        "     dst_path = categorized_dir_pos / pathlib.Path(row[\"fundus\"]).name\n",
        "\n",
        "  shutil.copyfile(src=src_path, dst=dst_path)"
      ],
      "metadata": {
        "id": "Cvw3TqHul73-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Negative count : {len([images for images in categorized_dir_neg.glob('*')])}\")\n",
        "print(f\"Positive count : {len([images for images in categorized_dir_pos.glob('*')])}\")"
      ],
      "metadata": {
        "id": "HAt2__QJxu-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2500 test, 1000 val, 1000 test\n",
        "\n",
        "cat_dir = pathlib.Path(\"Fundus\")\n",
        "new_base_dir = pathlib.Path(\"Glaucoma_Fundus_Small\")\n",
        "\n",
        "def makeSubset(subset_name, start_index, end_index):\n",
        "    for category in (\"Negative\", \"Positive\"):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "        src_filenames = [filename for filename in os.listdir(cat_dir / category) if filename.endswith(\".png\")]\n",
        "        for index, src_filenames in enumerate(src_filenames[start_index : end_index]):\n",
        "          dst_filename = f\"{category}.{index}.png\"\n",
        "          src_path = cat_dir / category / src_filenames\n",
        "          dst_path = dir / dst_filename\n",
        "          shutil.copyfile(src=src_path, dst=dst_path)\n",
        "\n",
        "makeSubset(\"train\", start_index=0, end_index=2500)\n",
        "makeSubset(\"validation\", start_index=2500, end_index=3500)\n",
        "makeSubset(\"test\", start_index=3500, end_index=4500)"
      ],
      "metadata": {
        "id": "Zvpx3sjvzbgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test folder Negative count: {len(list((new_base_dir / 'test' / 'Negative').glob('*')))}\")\n",
        "print(f\"Test folder Positive count: {len(list((new_base_dir / 'test' / 'Positive').glob('*')))}\")\n",
        "\n",
        "print(f\"Train folder Negative count: {len(list((new_base_dir / 'train' / 'Negative').glob('*')))}\")\n",
        "print(f\"Train folder Positive count: {len(list((new_base_dir / 'train' / 'Positive').glob('*')))}\")\n",
        "\n",
        "print(f\"Validation folder Negative count: {len(list((new_base_dir / 'validation' / 'Negative').glob('*')))}\")\n",
        "print(f\"Validation folder Positive count: {len(list((new_base_dir / 'validation' / 'Positive').glob('*')))}\")"
      ],
      "metadata": {
        "id": "R7nuOfhRtpJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "conv_base = InceptionV3(\n",
        "#  input_shape = (180, 180, 3),\n",
        " weights=\"imagenet\",\n",
        " include_top=False)\n",
        "\n",
        "# conv_base = keras.applications.vgg16.VGG16(\n",
        "#  weights=\"imagenet\",\n",
        "#  include_top=False)\n",
        "\n",
        "conv_base.trainable = False"
      ],
      "metadata": {
        "id": "29jt5R8NqcNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = keras.Sequential([\n",
        " layers.RandomFlip(\"horizontal\"),\n",
        " layers.RandomRotation(0.1),\n",
        " layers.RandomZoom(0.2),\n",
        " ])"
      ],
      "metadata": {
        "id": "I0hsASgTt-Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs = keras.Input(shape=(180, 180, 3))\n",
        "# x = data_augmentation(inputs)\n",
        "# x = layers.Rescaling(1./255)(x)\n",
        "# x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "# x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "# x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "# x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "# x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "# x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "# x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "# x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "# x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "# x = layers.Flatten()(x)\n",
        "# outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "# inputs = keras.Input(shape=(180, 180, 3))\n",
        "# x = data_augmentation(inputs)\n",
        "# x = layers.Rescaling(1./255)(x)\n",
        "# x = keras.applications.vgg16.preprocess_input(x)\n",
        "# x = conv_base(x)\n",
        "# x = layers.Flatten()(x)\n",
        "# x = layers.Dense(256)(x)\n",
        "# x = layers.Dropout(0.5)(x)\n",
        "# outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "# model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = conv_base(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "OI261rW1tMWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "BfHQDJghuItK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "LnBF9K4BuPEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        " new_base_dir / \"train\",\n",
        " image_size=(180, 180),\n",
        " batch_size=50)\n",
        "\n",
        "validation_dataset = image_dataset_from_directory(\n",
        " new_base_dir / \"validation\",\n",
        " image_size=(180, 180),\n",
        " batch_size=50)\n",
        "\n",
        "test_dataset = image_dataset_from_directory(\n",
        " new_base_dir / \"test\",\n",
        " image_size=(180, 180),\n",
        " batch_size=50)"
      ],
      "metadata": {
        "id": "473swOgRulq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        " keras.callbacks.ModelCheckpoint(\n",
        " filepath=\"feature_extraction_with_data_augmentation.keras\",\n",
        " save_best_only=True,\n",
        " monitor=\"val_loss\")\n",
        "]"
      ],
      "metadata": {
        "id": "HYBed-VruyC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        " train_dataset,\n",
        " epochs=30,\n",
        " validation_data=validation_dataset,\n",
        " callbacks=callbacks)"
      ],
      "metadata": {
        "id": "2Fbh6xcLuy6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "accuracy = history.history[\"accuracy\"]\n",
        "val_accuracy = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "On8PDU5c0KQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = keras.models.load_model(\"feature_extraction_with_data_augmentation.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "Psjm-Vuq0Lg1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}